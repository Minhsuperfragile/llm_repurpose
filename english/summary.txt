* ğŸ“š Retrieval Augmented Generation (RAG) often underperforms expectations due to limitations in retrieving and utilizing relevant information.

* ğŸ”  Vector search, while fast, loses information, causing relevant documents to be ranked lower than less relevant ones.

* â¬†ï¸ Increasing the number of retrieved documents (top_k) improves recall but exceeds LLMs' context window limits.

* âš ï¸  "Context stuffing" (exceeding the context window) harms LLM recall and performance.

* âš–ï¸ The solution: use rerankers to maximize retrieval recall (retrieve many documents) and then maximize LLM recall (select the most relevant documents for the LLM).

* ğŸ¤– Rerankers (cross-encoders) score query-document pairs, reordering documents by relevance.  This is a two-stage process: fast retrieval followed by slower, more accurate reranking.

* ğŸš€  Rerankers are slower than bi-encoders but far more accurate because they avoid information loss and consider query context.

* âš™ï¸  Bi-encoders pre-compute document embeddings, losing information and lacking query context. Rerankers process queries and documents together at query time, maintaining more information.

* ğŸ“Š Implementing two-stage retrieval involves embedding data with a bi-encoder (e.g., multilingual-e5-large), indexing it in a vector database (e.g., Pinecone), and then using a reranker (e.g., bge-reranker-v2-m3) to reorder results.

* âœ¨ Reranking significantly improves RAG performance by prioritizing highly relevant information, reducing noise for the LLM.
